{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf1cda2",
   "metadata": {},
   "source": [
    "# BW Journal Entry Anomaly Detection with Manual Clustering\n",
    "\n",
    "This notebook applies the **manual clustering methodology** from the Account_pairing notebook to the BW_sample_data_2021.csv dataset, followed by advanced anomaly detection techniques.\n",
    "\n",
    "## Key Methodology Steps:\n",
    "1. **Manual Account Clustering**: Create cluster patterns based on account pairings (Dr/Cr combinations)\n",
    "2. **Feature Engineering**: Timing analysis, amount patterns, and user behavior\n",
    "3. **JE-Level Aggregation**: Transform line-level data to journal entry level\n",
    "4. **Anomaly Detection**: Apply Isolation Forest and PCA within clusters\n",
    "5. **Business Rule Integration**: Combine ML with domain-specific risk factors\n",
    "\n",
    "This approach mirrors the sophisticated clustering strategy used in the original Account_pairing notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4d2a4",
   "metadata": {},
   "source": [
    "# BW Journal Entry Anomaly Detection\n",
    "\n",
    "This notebook applies anomaly detection techniques to the BW_sample_data_2021.csv dataset, adapting methods from the Account_pairing notebook.\n",
    "\n",
    "## Objectives:\n",
    "- Detect anomalous journal entries based on multiple risk factors\n",
    "- Feature engineering for timing, amounts, manual vs automatic entries, backposting, and user behavior\n",
    "- Apply Isolation Forest and PCA-based anomaly detection\n",
    "- Provide explainable results for audit and compliance teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a56f12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lime.tabular'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m silhouette_score\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtabular\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LimeTabularExplainer\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lime.tabular'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import shap\n",
    "from lime.tabular import LimeTabularExplainer\n",
    "from scipy import stats\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50e35e",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BW dataset\n",
    "df = pd.read_csv('BW_sample_data_2021.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type analysis and missing values\n",
    "print(\"Data Types and Missing Values:\")\n",
    "print(\"=\" * 50)\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Data_Type': df.dtypes,\n",
    "    'Non_Null_Count': df.count(),\n",
    "    'Null_Count': df.isnull().sum(),\n",
    "    'Null_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "print(info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce8662",
   "metadata": {},
   "source": [
    "## 2. Column Mapping and Feature Identification\n",
    "\n",
    "Based on the BW dataset structure, here are the identified columns for our features of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058451c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column mapping for features of interest\n",
    "COLUMN_MAPPING = {\n",
    "    # Journal Entry Identifiers\n",
    "    'je_header_id': 'B.JE_HEADER_ID',           # Journal Entry Header ID\n",
    "    'je_batch_id': 'B.JE_BATCH_ID',             # Journal Entry Batch ID\n",
    "    'je_line_num': 'A.JE_LINE_NUM',             # Line number within JE\n",
    "    \n",
    "    # Amounts\n",
    "    'entered_dr': 'A.ENTERED_DR',               # Debit amount (entered currency)\n",
    "    'entered_cr': 'A.ENTERED_CR',               # Credit amount (entered currency)\n",
    "    'accounted_dr': 'A.ACCOUNTED_DR',           # Debit amount (accounting currency)\n",
    "    'accounted_cr': 'A.ACCOUNTED_CR',           # Credit amount (accounting currency)\n",
    "    \n",
    "    # Timing fields\n",
    "    'posted_date': 'B.POSTED_DATE',             # When JE was posted\n",
    "    'creation_date': 'B.CREATION_DATE',         # When JE was created\n",
    "    'effective_date': 'B.DEFAULT_EFFECTIVE_DATE', # Effective date of JE\n",
    "    \n",
    "    # User information\n",
    "    'created_by': 'B.CREATED_BY',               # Person who created the JE\n",
    "    'last_updated_by': 'B.LAST_UPDATED_BY',     # Person who last updated\n",
    "    \n",
    "    # Entry type indicators\n",
    "    'je_source': 'B.JE_SOURCE',                 # Source (Manual vs Automatic)\n",
    "    'je_category': 'B.JE_CATEGORY',             # Category of JE\n",
    "    'status': 'B.STATUS',                       # Status (Posted, etc.)\n",
    "    \n",
    "    # Account information\n",
    "    'segment1': 'C.SEGMENT1',                   # Account segment 1 (main account)\n",
    "    'segment2': 'C.SEGMENT2',                   # Account segment 2\n",
    "    'segment3': 'C.SEGMENT3',                   # Account segment 3\n",
    "    'segment4': 'C.SEGMENT4',                   # Account segment 4\n",
    "    \n",
    "    # Descriptions\n",
    "    'je_name': 'B.NAME',                        # JE name\n",
    "    'je_description': 'B.DESCRIPTION',          # JE description\n",
    "    'line_description': 'A.DESCRIPTION',       # Line description\n",
    "    \n",
    "    # Currency\n",
    "    'currency_code': 'B.CURRENCY_CODE',         # Currency\n",
    "    'currency_rate': 'B.CURRENCY_CONVERSION_RATE' # Conversion rate\n",
    "}\n",
    "\n",
    "print(\"Column Mapping for Features of Interest:\")\n",
    "print(\"=\" * 50)\n",
    "for feature, column in COLUMN_MAPPING.items():\n",
    "    print(f\"{feature:20s} -> {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78c781",
   "metadata": {},
   "source": [
    "## 2.5 Manual Account Clustering (Adapted from Account_pairing.ipynb)\n",
    "\n",
    "This section implements the same manual clustering methodology used in the Account_pairing notebook:\n",
    "1. **Account normalization** - standardize account names\n",
    "2. **Account ID creation** - combine account numbers with cleaned names  \n",
    "3. **Cluster pattern generation** - create Dr/Cr combinations per journal entry\n",
    "4. **Pattern-based grouping** - group JEs by their account interaction patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bb3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account normalization function (from Account_pairing.ipynb)\n",
    "import re\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize account names for comparison (adapted from original)\"\"\"\n",
    "    name = str(name).lower()\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    # Add common accounting normalization rules\n",
    "    name = name.replace('capatalization', 'capitalization')\n",
    "    name = name.replace('(ntd)', '(twd)')\n",
    "    name = name.replace('&', 'and')\n",
    "    name = name.replace('acct', 'account')\n",
    "    return name\n",
    "\n",
    "# Test the function\n",
    "print(\"Account name normalization function ready!\")\n",
    "print(\"Example: 'CASH & Cash Equivalents' ->\", normalize_name('CASH & Cash Equivalents'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Account IDs by combining segments (Chart of Accounts structure)\n",
    "def create_account_ids_bw(df):\n",
    "    \"\"\"\n",
    "    Create Account IDs from BW dataset segments, adapting the approach from Account_pairing.ipynb\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify segment columns that exist in BW data\n",
    "    segment_cols = [col for col in df.columns if col.startswith('C.SEGMENT')]\n",
    "    print(f\"Found segment columns: {segment_cols}\")\n",
    "    \n",
    "    if len(segment_cols) >= 1:\n",
    "        # Use SEGMENT1 as primary account number (like No_GLAcc in original)\n",
    "        df['Account_Number'] = df['C.SEGMENT1'].fillna('UNKNOWN')\n",
    "        \n",
    "        # Create a synthetic account name from available segments\n",
    "        # In BW data, we don't have explicit account names, so we'll create them from segments\n",
    "        account_name_parts = []\n",
    "        for i, col in enumerate(segment_cols[:4]):  # Use first 4 segments\n",
    "            df[f'seg_{i+1}'] = df[col].fillna('').astype(str)\n",
    "            account_name_parts.append(f'seg_{i+1}')\n",
    "        \n",
    "        # Combine segments to create account name\n",
    "        df['Account_Name_Raw'] = df[account_name_parts].agg('-'.join, axis=1)\n",
    "        df['Account_Name_Raw'] = df['Account_Name_Raw'].str.replace('-+', '-', regex=True).str.strip('-')\n",
    "        \n",
    "        # Normalize the account names\n",
    "        df['Account_Name_Normalized'] = df['Account_Name_Raw'].apply(normalize_name)\n",
    "        \n",
    "        # Create Account_ID (number + normalized name)\n",
    "        df['Account_ID'] = df['Account_Number'].astype(str) + ' ' + df['Account_Name_Normalized']\n",
    "        \n",
    "        print(f\"Created {df['Account_ID'].nunique()} unique Account IDs\")\n",
    "        print(\"Sample Account IDs:\")\n",
    "        print(df['Account_ID'].value_counts().head())\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        temp_cols = [f'seg_{i+1}' for i in range(4)] + ['Account_Name_Raw', 'Account_Name_Normalized']\n",
    "        df = df.drop(columns=[col for col in temp_cols if col in df.columns])\n",
    "        \n",
    "    else:\n",
    "        print(\"Warning: No segment columns found. Using line descriptions as fallback.\")\n",
    "        # Fallback: use line description\n",
    "        df['Account_ID'] = df.get('A.DESCRIPTION', 'UNKNOWN_ACCOUNT').fillna('UNKNOWN_ACCOUNT')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply account ID creation\n",
    "print(\"Creating Account IDs from BW dataset...\")\n",
    "df_work = create_account_ids_bw(df_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575fbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Journal Entry Document IDs and Cluster Patterns (core of manual clustering)\n",
    "def label_journal_entries_by_pattern_cluster_bw(df):\n",
    "    \"\"\"\n",
    "    Adapted from Account_pairing.ipynb: Groups journal entries by JE document,\n",
    "    determines the full cluster pattern for each JE, and assigns cluster IDs.\n",
    "    \n",
    "    This is the CORE manual clustering methodology from the original notebook.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original\n",
    "    result_df = df.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Step 1: Create JE_Doc_ID to group entries (like in original)\n",
    "    # Using JE_HEADER_ID as the primary grouping key for BW data\n",
    "    result_df['JE_Doc_ID'] = result_df['je_header_id'].astype(str)\n",
    "    \n",
    "    # For additional uniqueness, we can add batch info if needed\n",
    "    if 'je_batch_id' in result_df.columns:\n",
    "        result_df['JE_Doc_ID'] = (\n",
    "            result_df['je_batch_id'].astype(str) + '|' +\n",
    "            result_df['je_header_id'].astype(str)\n",
    "        )\n",
    "    \n",
    "    # Step 2: Define cluster for each line (Dr/Cr + Account_ID)\n",
    "    def get_cluster_bw(row):\n",
    "        \"\"\"Determine if this line is a Debit, Credit, or Zero entry\"\"\"\n",
    "        debit_amt = row.get('entered_dr', 0) if pd.notnull(row.get('entered_dr', 0)) else 0\n",
    "        credit_amt = row.get('entered_cr', 0) if pd.notnull(row.get('entered_cr', 0)) else 0\n",
    "        \n",
    "        if debit_amt != 0 and credit_amt == 0:\n",
    "            direction = 'Dr'\n",
    "        elif credit_amt != 0 and debit_amt == 0:\n",
    "            direction = 'Cr'\n",
    "        elif debit_amt == 0 and credit_amt == 0:\n",
    "            direction = 'Zero'\n",
    "        else:\n",
    "            # Both non-zero (shouldn't happen in proper accounting)\n",
    "            direction = 'Both'\n",
    "            \n",
    "        account_id = row.get('Account_ID', 'UNKNOWN')\n",
    "        return f\"{direction} {account_id}\"\n",
    "    \n",
    "    result_df['Cluster'] = result_df.apply(get_cluster_bw, axis=1)\n",
    "    \n",
    "    # Step 3: For each JE_Doc_ID, get the sorted tuple of all its clusters (the pattern)\n",
    "    print(\"Creating cluster patterns for each Journal Entry...\")\n",
    "    je_pattern = (\n",
    "        result_df[['JE_Doc_ID', 'Cluster']]\n",
    "        .drop_duplicates()\n",
    "        .groupby('JE_Doc_ID')['Cluster']\n",
    "        .apply(lambda x: tuple(sorted(x)))  # Normalize order for consistent hashing\n",
    "        .reset_index()\n",
    "        .rename(columns={'Cluster': 'ClusterPattern'})\n",
    "    )\n",
    "    \n",
    "    # Step 4: Assign a unique PatternCluster_ID to each unique cluster pattern\n",
    "    unique_patterns = je_pattern['ClusterPattern'].drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"Found {len(unique_patterns)} unique cluster patterns\")\n",
    "    \n",
    "    pattern_to_id = {\n",
    "        pattern: f\"Cluster_{idx+1}\" for idx, pattern in enumerate(unique_patterns)\n",
    "    }\n",
    "    \n",
    "    # Map back to JE_Doc_ID -> Cluster_ID\n",
    "    je_pattern['Cluster_ID'] = je_pattern['ClusterPattern'].apply(lambda x: pattern_to_id.get(x, 'Unknown'))\n",
    "    \n",
    "    # Step 5: Merge Cluster_ID back into the main DataFrame\n",
    "    result_df = result_df.merge(\n",
    "        je_pattern[['JE_Doc_ID', 'Cluster_ID', 'ClusterPattern']], \n",
    "        on='JE_Doc_ID', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully assigned {result_df['Cluster_ID'].nunique()} cluster IDs\")\n",
    "    print(\"Top 10 most common cluster patterns:\")\n",
    "    print(result_df['Cluster_ID'].value_counts().head(10))\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Apply the manual clustering (this is the key step!)\n",
    "print(\"Applying manual clustering methodology from Account_pairing.ipynb...\")\n",
    "df_clustered = label_journal_entries_by_pattern_cluster_bw(df_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week-of-Month calculation (adapted from Account_pairing.ipynb)\n",
    "def get_week_of_month_4week_smart_merge(date):\n",
    "    \"\"\"\n",
    "    Smart 4-week month division that handles 5-6 calendar week months.\n",
    "    Adapted from Account_pairing.ipynb with same logic.\n",
    "    \"\"\"\n",
    "    dt = pd.to_datetime(date)\n",
    "    year, month = dt.year, dt.month\n",
    "\n",
    "    # Generate all dates in the month\n",
    "    start_of_month = dt.replace(day=1)\n",
    "    if month == 12:\n",
    "        end_of_month = dt.replace(year=year + 1, month=1, day=1) - pd.Timedelta(days=1)\n",
    "    else:\n",
    "        end_of_month = dt.replace(month=month + 1, day=1) - pd.Timedelta(days=1)\n",
    "\n",
    "    dates_in_month = pd.date_range(start=start_of_month, end=end_of_month, freq='D')\n",
    "\n",
    "    # GET MONDAY AS WEEK START USING Monday-based weeks\n",
    "    monday_starts = pd.Series(dates_in_month).apply(lambda d: d - pd.Timedelta(days=d.weekday()))\n",
    "\n",
    "    # Group days in month by their Monday week start\n",
    "    week_groups = {}\n",
    "    for d, ws in zip(dates_in_month, monday_starts):\n",
    "        if ws not in week_groups:\n",
    "            week_groups[ws] = []\n",
    "        week_groups[ws].append(d.day)\n",
    "\n",
    "    # Sort the unique Monday starts\n",
    "    sorted_week_starts = sorted(week_groups.keys())\n",
    "    n_weeks = len(sorted_week_starts)\n",
    "\n",
    "    # Final assignment: Monday start → (week_num, days_in_merged_week)\n",
    "    assignment = {}\n",
    "\n",
    "    if n_weeks == 4:\n",
    "        for i, ws in enumerate(sorted_week_starts):\n",
    "            assignment[ws] = (i + 1, len(week_groups[ws]))\n",
    "\n",
    "    elif n_weeks == 5:\n",
    "        first_len = len(week_groups[sorted_week_starts[0]])\n",
    "        last_len = len(week_groups[sorted_week_starts[-1]])\n",
    "\n",
    "        if first_len <= last_len:  # Merge first into second → Week 1\n",
    "            w1_days = first_len + len(week_groups[sorted_week_starts[1]])\n",
    "            assignment[sorted_week_starts[0]] = (1, w1_days)\n",
    "            assignment[sorted_week_starts[1]] = (1, w1_days)\n",
    "            assignment[sorted_week_starts[2]] = (2, len(week_groups[sorted_week_starts[2]]))\n",
    "            assignment[sorted_week_starts[3]] = (3, len(week_groups[sorted_week_starts[3]]))\n",
    "            assignment[sorted_week_starts[4]] = (4, len(week_groups[sorted_week_starts[4]]))\n",
    "        else:  # Merge last into fourth → Week 4\n",
    "            w4_days = len(week_groups[sorted_week_starts[3]]) + last_len\n",
    "            assignment[sorted_week_starts[0]] = (1, len(week_groups[sorted_week_starts[0]]))\n",
    "            assignment[sorted_week_starts[1]] = (2, len(week_groups[sorted_week_starts[1]]))\n",
    "            assignment[sorted_week_starts[2]] = (3, len(week_groups[sorted_week_starts[2]]))\n",
    "            assignment[sorted_week_starts[3]] = (4, w4_days)\n",
    "            assignment[sorted_week_starts[4]] = (4, w4_days)\n",
    "\n",
    "    elif n_weeks == 6:\n",
    "        # Fold: W1→W2 → Week 1; W6→W5 → Week 4\n",
    "        w1_days = len(week_groups[sorted_week_starts[0]]) + len(week_groups[sorted_week_starts[1]])\n",
    "        w4_days = len(week_groups[sorted_week_starts[4]]) + len(week_groups[sorted_week_starts[5]])\n",
    "        \n",
    "        assignment[sorted_week_starts[0]] = (1, w1_days)\n",
    "        assignment[sorted_week_starts[1]] = (1, w1_days)\n",
    "        assignment[sorted_week_starts[2]] = (2, len(week_groups[sorted_week_starts[2]]))\n",
    "        assignment[sorted_week_starts[3]] = (3, len(week_groups[sorted_week_starts[3]]))\n",
    "        assignment[sorted_week_starts[4]] = (4, w4_days)\n",
    "        assignment[sorted_week_starts[5]] = (4, w4_days)\n",
    "\n",
    "    else:\n",
    "        # Fallback\n",
    "        total_days = len(dates_in_month)\n",
    "        sizes = [total_days // 4 + (1 if i < total_days % 4 else 0) for i in range(4)]\n",
    "        for i, ws in enumerate(sorted_week_starts):\n",
    "            bucket = min(i, 3)\n",
    "            assignment[ws] = (bucket + 1, sizes[bucket])\n",
    "\n",
    "    # Find input date's Monday week start\n",
    "    input_monday = dt - pd.Timedelta(days=dt.weekday())\n",
    "    week_num, days_in_week = assignment.get(input_monday, (4, 7))\n",
    "\n",
    "    return {\n",
    "        'week_of_month': week_num,\n",
    "        'days_in_week': days_in_week\n",
    "    }\n",
    "\n",
    "# Apply week-of-month calculation to the posted dates\n",
    "print(\"Calculating week-of-month for timing analysis...\")\n",
    "if 'posted_date' in df_clustered.columns:\n",
    "    week_info = df_clustered['posted_date'].apply(get_week_of_month_4week_smart_merge)\n",
    "    df_clustered['WeekOfMonth'] = [info['week_of_month'] for info in week_info]\n",
    "    df_clustered['Days_in_week'] = [info['days_in_week'] for info in week_info]\n",
    "    \n",
    "    print(\"Week of Month Distribution:\")\n",
    "    print(df_clustered['WeekOfMonth'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"Warning: No posted_date column found for week calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly JE Frequency Calculation (adapted from Account_pairing.ipynb)\n",
    "def get_weekly_je_frequency_by_clusterpattern_bw(df):\n",
    "    \"\"\"\n",
    "    For each ClusterPattern, compute the count and frequency of \n",
    "    Journal Entries (JE_Doc_ID) per WeekOfMonth, normalized by Days_in_week.\n",
    "    Adapted from Account_pairing.ipynb methodology.\n",
    "    \"\"\"\n",
    "    # Step 1: Drop duplicate JEs per cluster and week\n",
    "    je_week = df[['Cluster_ID', 'JE_Doc_ID', 'WeekOfMonth', 'Days_in_week']].drop_duplicates(\n",
    "        subset=['Cluster_ID', 'JE_Doc_ID']\n",
    "    )\n",
    "\n",
    "    # Step 2: Aggregate JE count and average Days_in_week per cluster-week\n",
    "    weekly_counts = (\n",
    "        je_week.groupby(['Cluster_ID', 'WeekOfMonth'])\n",
    "        .agg(\n",
    "            je_count=('JE_Doc_ID', 'count'),\n",
    "            avg_days_in_week=('Days_in_week', 'mean')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Step 3: Calculate frequency normalized by days\n",
    "    weekly_counts['je_frequency'] = weekly_counts['je_count'] / weekly_counts['avg_days_in_week']\n",
    "\n",
    "    # Total freq per cluster\n",
    "    total_freq = (\n",
    "        weekly_counts.groupby('Cluster_ID')['je_frequency'].sum()\n",
    "        .reset_index(name='total_cluster_frequency')\n",
    "    )\n",
    "\n",
    "    weekly_counts = weekly_counts.merge(total_freq, on='Cluster_ID')\n",
    "    weekly_counts['je_%_frequency'] = (weekly_counts['je_frequency'] / weekly_counts['total_cluster_frequency']) * 100\n",
    "\n",
    "    return weekly_counts\n",
    "\n",
    "# Calculate weekly frequencies\n",
    "print(\"Calculating weekly JE frequencies by cluster pattern...\")\n",
    "if 'WeekOfMonth' in df_clustered.columns:\n",
    "    weekly_freq_df = get_weekly_je_frequency_by_clusterpattern_bw(df_clustered)\n",
    "    \n",
    "    # Merge frequency data back to main dataframe\n",
    "    merge_cols = ['Cluster_ID', 'WeekOfMonth']\n",
    "    df_clustered = df_clustered.merge(\n",
    "        weekly_freq_df[merge_cols + ['je_frequency', 'je_%_frequency']],\n",
    "        on=merge_cols,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Create infrequent week indicator (same logic as original)\n",
    "    cluster_freq_stats = weekly_freq_df.groupby('Cluster_ID')['je_%_frequency'].agg(['mean', 'std']).reset_index()\n",
    "    cluster_freq_stats['freq_threshold'] = cluster_freq_stats['mean'] * 0.5  # 50% of mean frequency\n",
    "    \n",
    "    df_clustered = df_clustered.merge(\n",
    "        cluster_freq_stats[['Cluster_ID', 'freq_threshold']],\n",
    "        on='Cluster_ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    df_clustered['infrequent_w'] = (df_clustered['je_%_frequency'] < df_clustered['freq_threshold']).astype(int)\n",
    "    \n",
    "    print(\"Weekly frequency analysis complete!\")\n",
    "    print(f\"Infrequent week entries: {df_clustered['infrequent_w'].sum()}\")\n",
    "else:\n",
    "    print(\"Warning: Week calculation required for frequency analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trailing Zeros Analysis (adapted from Account_pairing.ipynb)\n",
    "def trailing_zeros(n):\n",
    "    \"\"\"Count trailing zeros in a number (fraud detection indicator)\"\"\"\n",
    "    if pd.isna(n) or n == 0:\n",
    "        return 0\n",
    "    whole_part = str(int(float(n)))\n",
    "    count = 0\n",
    "    for digit in reversed(whole_part):\n",
    "        if digit == '0':\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    return count\n",
    "\n",
    "def check_trailing_zeros_bw(df, debit_col, credit_col):\n",
    "    \"\"\"\n",
    "    Check trailing zeros in amounts (adapted from Account_pairing.ipynb)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    df[debit_col] = pd.to_numeric(df[debit_col], errors='coerce').fillna(0)\n",
    "    df[credit_col] = pd.to_numeric(df[credit_col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Identify rows where both columns are non-zero (unusual)\n",
    "    both_non_zero = df[(df[debit_col] != 0) & (df[credit_col] != 0)]\n",
    "    if not both_non_zero.empty:\n",
    "        print(f\"⚠️ Warning: Found {len(both_non_zero)} rows where both debit and credit are non-zero\")\n",
    "    \n",
    "    # Filter rows where exactly one column is non-zero\n",
    "    valid_entries = df[((df[debit_col] != 0) & (df[credit_col] == 0)) | \n",
    "                      ((df[debit_col] == 0) & (df[credit_col] != 0))].copy()\n",
    "    \n",
    "    # Create a column with the non-zero value\n",
    "    valid_entries['line_amount'] = valid_entries.apply(\n",
    "        lambda row: row[debit_col] if row[debit_col] != 0 else row[credit_col], axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate trailing zeros\n",
    "    valid_entries['trailing_zeros'] = valid_entries['line_amount'].apply(trailing_zeros)\n",
    "    \n",
    "    # Merge back to original dataframe\n",
    "    df = df.merge(\n",
    "        valid_entries[['trailing_zeros']],\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    df['trailing_zeros'] = df['trailing_zeros'].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply trailing zeros analysis\n",
    "print(\"Analyzing trailing zeros in amounts...\")\n",
    "if 'entered_dr' in df_clustered.columns and 'entered_cr' in df_clustered.columns:\n",
    "    df_clustered = check_trailing_zeros_bw(df_clustered, 'entered_dr', 'entered_cr')\n",
    "    \n",
    "    print(\"Trailing zeros distribution:\")\n",
    "    print(df_clustered['trailing_zeros'].value_counts().sort_index())\n",
    "    \n",
    "    # Highlight high trailing zeros (potential fraud indicator)\n",
    "    high_trailing = df_clustered[df_clustered['trailing_zeros'] >= 3]\n",
    "    print(f\"\\\\nEntries with 3+ trailing zeros: {len(high_trailing)} (potential fraud indicators)\")\n",
    "else:\n",
    "    print(\"Warning: Debit/Credit columns not found for trailing zeros analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JE-Level Summary (adapted from Account_pairing.ipynb)\n",
    "def summarize_je_amounts_bw(df):\n",
    "    \"\"\"\n",
    "    Create JE-level summaries matching the Account_pairing.ipynb methodology\n",
    "    \"\"\"\n",
    "    # Group by JE_Doc_ID to create journal entry level summaries\n",
    "    je_summary = df.groupby('JE_Doc_ID').agg({\n",
    "        # Amount summaries\n",
    "        'entered_dr': 'sum',\n",
    "        'entered_cr': 'sum', \n",
    "        'line_amount': 'sum',\n",
    "        'trailing_zeros': 'sum',\n",
    "        \n",
    "        # Cluster information (take first since they're the same within JE)\n",
    "        'Cluster_ID': 'first',\n",
    "        'ClusterPattern': 'first',\n",
    "        \n",
    "        # Timing information  \n",
    "        'posted_date': 'first',\n",
    "        'WeekOfMonth': 'first',\n",
    "        'Days_in_week': 'first',\n",
    "        'je_frequency': 'first',\n",
    "        'je_%_frequency': 'first',\n",
    "        'infrequent_w': 'first',\n",
    "        \n",
    "        # User information\n",
    "        'created_by': 'first',\n",
    "        'je_source': 'first',\n",
    "        \n",
    "        # Count of line items per JE\n",
    "        'je_line_num': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns to match original methodology\n",
    "    je_summary = je_summary.rename(columns={\n",
    "        'entered_dr': 'Total_Debit',\n",
    "        'entered_cr': 'Total_Credit', \n",
    "        'line_amount': 'Total_JE_Amount',\n",
    "        'trailing_zeros': 'Total_JE_Amt_trailing0s',\n",
    "        'je_line_num': 'Line_Count'\n",
    "    })\n",
    "    \n",
    "    # Calculate additional features like in original\n",
    "    je_summary['Amount_Imbalance'] = abs(je_summary['Total_Debit'] - je_summary['Total_Credit'])\n",
    "    \n",
    "    print(f\"Created JE-level summary with {len(je_summary)} journal entries\")\n",
    "    print(\"JE Summary Statistics:\")\n",
    "    print(je_summary[['Total_JE_Amount', 'Total_JE_Amt_trailing0s', 'Line_Count']].describe())\n",
    "    \n",
    "    return je_summary\n",
    "\n",
    "# Create JE-level summary dataframe\n",
    "print(\"Creating JE-level summary (matching Account_pairing.ipynb approach)...\")\n",
    "je_summary_df = summarize_je_amounts_bw(df_clustered)\n",
    "\n",
    "# Display sample of the summarized data\n",
    "print(\"\\\\nSample of JE-level data:\")\n",
    "display_cols = ['JE_Doc_ID', 'Cluster_ID', 'Total_JE_Amount', 'Total_JE_Amt_trailing0s', \n",
    "               'WeekOfMonth', 'infrequent_w', 'Line_Count']\n",
    "available_display_cols = [col for col in display_cols if col in je_summary_df.columns]\n",
    "print(je_summary_df[available_display_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847159e",
   "metadata": {},
   "source": [
    "## 3. Additional Feature Engineering (Building on Manual Clustering)\n",
    "\n",
    "Now that we have applied the manual clustering methodology from Account_pairing.ipynb, we'll add supplementary features for enhanced anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ecdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the clustered data as our working dataset\n",
    "df_work = df_clustered.copy()\n",
    "\n",
    "# Rename columns for easier access (using our previously established mapping)\n",
    "reverse_mapping = {v: k for k, v in COLUMN_MAPPING.items()}\n",
    "# Only rename columns that exist and aren't already renamed\n",
    "for original_col, new_col in reverse_mapping.items():\n",
    "    if original_col in df_work.columns and new_col not in df_work.columns:\n",
    "        df_work = df_work.rename(columns={original_col: new_col})\n",
    "\n",
    "print(\"Working with clustered dataset:\")\n",
    "print(f\"Shape: {df_work.shape}\")\n",
    "print(f\"Unique Cluster IDs: {df_work['Cluster_ID'].nunique()}\")\n",
    "print(f\"Unique JE Documents: {df_work['JE_Doc_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c130e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "date_columns = ['posted_date', 'creation_date', 'effective_date']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in df_work.columns:\n",
    "        # Handle different date formats that might exist\n",
    "        df_work[col] = pd.to_datetime(df_work[col], errors='coerce')\n",
    "        print(f\"{col}: {df_work[col].dtype}, Non-null: {df_work[col].count()}\")\n",
    "\n",
    "# Display date range\n",
    "if 'posted_date' in df_work.columns:\n",
    "    print(f\"\\nPosted Date Range: {df_work['posted_date'].min()} to {df_work['posted_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ddab9",
   "metadata": {},
   "source": [
    "### 3.1 Feature Engineering: Journal Entry Amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_amount_features(df):\n",
    "    \"\"\"\n",
    "    Engineer amount-related features for anomaly detection\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Fill NaN amounts with 0\n",
    "    amount_cols = ['entered_dr', 'entered_cr', 'accounted_dr', 'accounted_cr']\n",
    "    for col in amount_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Calculate line-level amounts\n",
    "    if 'entered_dr' in df.columns and 'entered_cr' in df.columns:\n",
    "        df['line_amount'] = df['entered_dr'] + df['entered_cr']\n",
    "    \n",
    "    if 'accounted_dr' in df.columns and 'accounted_cr' in df.columns:\n",
    "        df['line_amount_base'] = df['accounted_dr'] + df['accounted_cr']\n",
    "    \n",
    "    # Count trailing zeros in amounts (fraud indicator)\n",
    "    def count_trailing_zeros(amount):\n",
    "        if pd.isna(amount) or amount == 0:\n",
    "            return 0\n",
    "        amount_str = f\"{amount:.2f}\".rstrip('0').rstrip('.')\n",
    "        original_str = f\"{amount:.2f}\"\n",
    "        return len(original_str) - len(amount_str)\n",
    "    \n",
    "    if 'line_amount' in df.columns:\n",
    "        df['trailing_zeros'] = df['line_amount'].apply(count_trailing_zeros)\n",
    "    \n",
    "    # Benford's Law analysis (first digit)\n",
    "    def first_digit(amount):\n",
    "        if pd.isna(amount) or amount <= 0:\n",
    "            return None\n",
    "        return int(str(int(abs(amount)))[0])\n",
    "    \n",
    "    if 'line_amount' in df.columns:\n",
    "        df['first_digit'] = df['line_amount'].apply(first_digit)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply amount feature engineering\n",
    "df_work = engineer_amount_features(df_work)\n",
    "\n",
    "print(\"Amount Features Created:\")\n",
    "amount_features = ['line_amount', 'line_amount_base', 'trailing_zeros', 'first_digit']\n",
    "for feature in amount_features:\n",
    "    if feature in df_work.columns:\n",
    "        print(f\"{feature}: {df_work[feature].describe()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1358b",
   "metadata": {},
   "source": [
    "### 3.2 Feature Engineering: Timing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeef52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_timing_features(df):\n",
    "    \"\"\"\n",
    "    Engineer timing-related features for anomaly detection\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract timing components from posted_date\n",
    "    if 'posted_date' in df.columns:\n",
    "        df['posted_year'] = df['posted_date'].dt.year\n",
    "        df['posted_month'] = df['posted_date'].dt.month\n",
    "        df['posted_day'] = df['posted_date'].dt.day\n",
    "        df['posted_weekday'] = df['posted_date'].dt.weekday  # 0=Monday, 6=Sunday\n",
    "        df['posted_hour'] = df['posted_date'].dt.hour\n",
    "        \n",
    "        # Period-end indicators (common manipulation periods)\n",
    "        df['is_month_end'] = (df['posted_date'].dt.day >= 28)  # Last few days of month\n",
    "        df['is_quarter_end'] = df['posted_month'].isin([3, 6, 9, 12]) & df['is_month_end']\n",
    "        df['is_year_end'] = (df['posted_month'] == 12) & df['is_month_end']\n",
    "        \n",
    "        # Weekend/after-hours posting (unusual timing)\n",
    "        df['is_weekend'] = df['posted_weekday'].isin([5, 6])  # Saturday, Sunday\n",
    "        df['is_after_hours'] = (df['posted_hour'] < 8) | (df['posted_hour'] > 18)\n",
    "        df['is_unusual_timing'] = df['is_weekend'] | df['is_after_hours']\n",
    "    \n",
    "    # Backposting detection (posting date after effective date)\n",
    "    if 'posted_date' in df.columns and 'effective_date' in df.columns:\n",
    "        df['is_backposted'] = df['posted_date'] > df['effective_date']\n",
    "        df['backpost_days'] = (df['posted_date'] - df['effective_date']).dt.days\n",
    "        df['backpost_days'] = df['backpost_days'].fillna(0)\n",
    "    \n",
    "    # Time between creation and posting\n",
    "    if 'creation_date' in df.columns and 'posted_date' in df.columns:\n",
    "        df['creation_to_post_hours'] = (df['posted_date'] - df['creation_date']).dt.total_seconds() / 3600\n",
    "        df['creation_to_post_hours'] = df['creation_to_post_hours'].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply timing feature engineering\n",
    "df_work = engineer_timing_features(df_work)\n",
    "\n",
    "print(\"Timing Features Created:\")\n",
    "timing_features = ['is_month_end', 'is_quarter_end', 'is_year_end', 'is_weekend', \n",
    "                  'is_after_hours', 'is_unusual_timing', 'is_backposted', 'backpost_days']\n",
    "for feature in timing_features:\n",
    "    if feature in df_work.columns:\n",
    "        value_counts = df_work[feature].value_counts()\n",
    "        print(f\"{feature}: {value_counts}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868d4a0",
   "metadata": {},
   "source": [
    "### 3.3 Feature Engineering: Entry Type and User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_entry_type_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features related to entry types and user behavior\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Manual vs Automatic entry detection\n",
    "    if 'je_source' in df.columns:\n",
    "        # Common patterns for manual vs automatic\n",
    "        manual_indicators = ['Manual', 'Spreadsheet', 'Web Entry', 'Journal Entry']\n",
    "        auto_indicators = ['AutoPost', 'Subledger', 'Import', 'Interface']\n",
    "        \n",
    "        df['is_manual_entry'] = df['je_source'].str.contains('|'.join(manual_indicators), case=False, na=False)\n",
    "        df['is_auto_entry'] = df['je_source'].str.contains('|'.join(auto_indicators), case=False, na=False)\n",
    "        \n",
    "        print(\"JE Source Analysis:\")\n",
    "        print(df['je_source'].value_counts())\n",
    "    \n",
    "    # Reversal entry detection\n",
    "    if 'je_name' in df.columns:\n",
    "        reversal_keywords = ['Reverse', 'Reversal', 'REV']\n",
    "        df['is_reversal'] = df['je_name'].str.contains('|'.join(reversal_keywords), case=False, na=False)\n",
    "        \n",
    "    if 'je_description' in df.columns:\n",
    "        df['is_reversal'] = df['is_reversal'] | df['je_description'].str.contains('|'.join(reversal_keywords), case=False, na=False)\n",
    "    \n",
    "    # User frequency analysis (detect unusual users)\n",
    "    if 'created_by' in df.columns:\n",
    "        user_counts = df['created_by'].value_counts()\n",
    "        df['user_frequency'] = df['created_by'].map(user_counts)\n",
    "        df['is_infrequent_user'] = df['user_frequency'] < user_counts.quantile(0.1)  # Bottom 10%\n",
    "        \n",
    "        print(f\"\\nUser Analysis:\")\n",
    "        print(f\"Total unique users: {df['created_by'].nunique()}\")\n",
    "        print(f\"Top 5 users by volume:\")\n",
    "        print(user_counts.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply entry type feature engineering\n",
    "df_work = engineer_entry_type_features(df_work)\n",
    "\n",
    "print(\"\\nEntry Type Features Created:\")\n",
    "entry_features = ['is_manual_entry', 'is_auto_entry', 'is_reversal', 'is_infrequent_user']\n",
    "for feature in entry_features:\n",
    "    if feature in df_work.columns:\n",
    "        value_counts = df_work[feature].value_counts()\n",
    "        print(f\"{feature}: {value_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7691705",
   "metadata": {},
   "source": [
    "### 3.4 Account Pairing Clusters (adapted from original notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_account_clusters(df):\n",
    "    \"\"\"\n",
    "    Create account pairing clusters based on the segments (Chart of Accounts structure)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create account ID from segments\n",
    "    segment_cols = [col for col in df.columns if col.startswith('segment') and col in df.columns]\n",
    "    \n",
    "    if segment_cols:\n",
    "        # Combine segments to create account ID\n",
    "        df['account_id'] = df[segment_cols].fillna('').astype(str).agg('-'.join, axis=1)\n",
    "        df['account_id'] = df['account_id'].str.replace('-+', '-', regex=True).str.strip('-')\n",
    "        \n",
    "        # Create debit/credit patterns per JE\n",
    "        def create_cluster_pattern(group):\n",
    "            patterns = []\n",
    "            for _, row in group.iterrows():\n",
    "                if row.get('entered_dr', 0) > 0:\n",
    "                    patterns.append(f\"Dr_{row['account_id']}\")\n",
    "                if row.get('entered_cr', 0) > 0:\n",
    "                    patterns.append(f\"Cr_{row['account_id']}\")\n",
    "            return tuple(sorted(patterns))\n",
    "        \n",
    "        # Group by JE Header ID to create cluster patterns\n",
    "        if 'je_header_id' in df.columns:\n",
    "            cluster_patterns = df.groupby('je_header_id').apply(create_cluster_pattern)\n",
    "            cluster_patterns.name = 'cluster_pattern'\n",
    "            \n",
    "            # Map back to original dataframe\n",
    "            df = df.merge(cluster_patterns.reset_index(), on='je_header_id', how='left')\n",
    "            \n",
    "            print(f\"Created {cluster_patterns.nunique()} unique account cluster patterns\")\n",
    "            print(f\"Top 5 most common patterns:\")\n",
    "            print(cluster_patterns.value_counts().head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply account clustering\n",
    "df_work = create_account_clusters(df_work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6cb79",
   "metadata": {},
   "source": [
    "### 3.5 Journal Entry Level Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f831c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the JE-level summary we created with manual clustering\n",
    "je_df = je_summary_df.copy()\n",
    "\n",
    "print(f\"Using JE-level dataset from manual clustering:\")\n",
    "print(f\"Shape: {je_df.shape}\")\n",
    "print(f\"Columns: {list(je_df.columns)}\")\n",
    "\n",
    "# Add any additional features that weren't captured in the summary\n",
    "additional_features = {}\n",
    "\n",
    "# Add timing features if not already present\n",
    "if 'posted_date' in je_df.columns:\n",
    "    if 'posted_year' not in je_df.columns:\n",
    "        je_df['posted_year'] = pd.to_datetime(je_df['posted_date']).dt.year\n",
    "    if 'posted_month' not in je_df.columns:\n",
    "        je_df['posted_month'] = pd.to_datetime(je_df['posted_date']).dt.month\n",
    "    if 'posted_weekday' not in je_df.columns:\n",
    "        je_df['posted_weekday'] = pd.to_datetime(je_df['posted_date']).dt.weekday\n",
    "    if 'posted_hour' not in je_df.columns:\n",
    "        je_df['posted_hour'] = pd.to_datetime(je_df['posted_date']).dt.hour\n",
    "        \n",
    "    # Period-end indicators\n",
    "    if 'is_month_end' not in je_df.columns:\n",
    "        je_df['is_month_end'] = (pd.to_datetime(je_df['posted_date']).dt.day >= 28)\n",
    "    if 'is_quarter_end' not in je_df.columns:\n",
    "        je_df['is_quarter_end'] = je_df['posted_month'].isin([3, 6, 9, 12]) & je_df['is_month_end']\n",
    "    if 'is_year_end' not in je_df.columns:\n",
    "        je_df['is_year_end'] = (je_df['posted_month'] == 12) & je_df['is_month_end']\n",
    "        \n",
    "    # Unusual timing\n",
    "    if 'is_weekend' not in je_df.columns:\n",
    "        je_df['is_weekend'] = je_df['posted_weekday'].isin([5, 6])\n",
    "    if 'is_after_hours' not in je_df.columns:\n",
    "        je_df['is_after_hours'] = (je_df['posted_hour'] < 8) | (je_df['posted_hour'] > 18)\n",
    "    if 'is_unusual_timing' not in je_df.columns:\n",
    "        je_df['is_unusual_timing'] = je_df['is_weekend'] | je_df['is_after_hours']\n",
    "\n",
    "# Manual vs automatic detection\n",
    "if 'je_source' in je_df.columns:\n",
    "    manual_indicators = ['Manual', 'Spreadsheet', 'Web Entry', 'Journal Entry']\n",
    "    auto_indicators = ['AutoPost', 'Subledger', 'Import', 'Interface']\n",
    "    \n",
    "    je_df['is_manual_entry'] = je_df['je_source'].str.contains('|'.join(manual_indicators), case=False, na=False)\n",
    "    je_df['is_auto_entry'] = je_df['je_source'].str.contains('|'.join(auto_indicators), case=False, na=False)\n",
    "\n",
    "print(f\"\\\\nFinal JE-level dataset ready for anomaly detection:\")\n",
    "print(f\"Shape: {je_df.shape}\")\n",
    "print(f\"Key features: Total_JE_Amount, Total_JE_Amt_trailing0s, infrequent_w, Cluster_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d44fd",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31d3a7",
   "metadata": {},
   "source": [
    "### 4.1 Feature Selection for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7498aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_for_modeling(df):\n",
    "    \"\"\"\n",
    "    Select and prepare features for anomaly detection models\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Select numeric features for modeling\n",
    "    numeric_features = []\n",
    "    boolean_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['float64', 'int64']:\n",
    "            if col.endswith(('_sum', '_count', '_mean', '_max', '_std')):\n",
    "                numeric_features.append(col)\n",
    "        elif df[col].dtype == 'bool' or col.startswith('is_'):\n",
    "            boolean_features.append(col)\n",
    "    \n",
    "    # Combine features\n",
    "    model_features = numeric_features + boolean_features\n",
    "    \n",
    "    # Filter out features with too many missing values or zero variance\n",
    "    valid_features = []\n",
    "    for feature in model_features:\n",
    "        if feature in df.columns:\n",
    "            non_null_pct = df[feature].count() / len(df)\n",
    "            if non_null_pct > 0.5:  # At least 50% non-null\n",
    "                if df[feature].std() > 0:  # Has variance\n",
    "                    valid_features.append(feature)\n",
    "    \n",
    "    print(f\"Selected {len(valid_features)} features for modeling:\")\n",
    "    for i, feature in enumerate(valid_features, 1):\n",
    "        print(f\"{i:2d}. {feature}\")\n",
    "    \n",
    "    # Prepare feature matrix\n",
    "    X = df[valid_features].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # Convert boolean to numeric\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'bool':\n",
    "            X[col] = X[col].astype(int)\n",
    "    \n",
    "    return X, valid_features\n",
    "\n",
    "# Prepare features\n",
    "X, feature_names = prepare_features_for_modeling(je_df)\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Feature matrix summary:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5162a5",
   "metadata": {},
   "source": [
    "### 4.2 Isolation Forest Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e211a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for anomaly detection (adapted from Account_pairing.ipynb methodology)\n",
    "def prepare_features_for_modeling_clustered(df):\n",
    "    \"\"\"\n",
    "    Select features for anomaly detection, focusing on the key features \n",
    "    used in Account_pairing.ipynb: Total_JE_Amount, infrequent_w, Total_JE_Amt_trailing0s\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Core features from Account_pairing.ipynb\n",
    "    core_features = [\n",
    "        'Total_JE_Amount',           # Main amount indicator\n",
    "        'infrequent_w',              # Timing anomaly indicator  \n",
    "        'Total_JE_Amt_trailing0s'    # Fraud indicator (round numbers)\n",
    "    ]\n",
    "    \n",
    "    # Additional features for enhanced detection\n",
    "    supplementary_features = [\n",
    "        'Line_Count',                # Number of lines in JE\n",
    "        'Amount_Imbalance',          # Debit/Credit imbalance\n",
    "        'WeekOfMonth',               # Timing pattern\n",
    "        'je_%_frequency'             # Frequency within cluster\n",
    "    ]\n",
    "    \n",
    "    # Boolean timing features\n",
    "    timing_features = [\n",
    "        'is_month_end', 'is_quarter_end', 'is_year_end',\n",
    "        'is_unusual_timing', 'is_weekend', 'is_after_hours'\n",
    "    ]\n",
    "    \n",
    "    # Entry type features\n",
    "    entry_features = [\n",
    "        'is_manual_entry', 'is_auto_entry'\n",
    "    ]\n",
    "    \n",
    "    # Combine all available features\n",
    "    all_features = core_features + supplementary_features + timing_features + entry_features\n",
    "    available_features = [f for f in all_features if f in df.columns]\n",
    "    \n",
    "    print(f\"Selected {len(available_features)} features for anomaly detection:\")\n",
    "    for i, feature in enumerate(available_features, 1):\n",
    "        print(f\"{i:2d}. {feature}\")\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = df[available_features].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # Convert boolean to numeric\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'bool':\n",
    "            X[col] = X[col].astype(int)\n",
    "    \n",
    "    # Handle infinite values\n",
    "    X = X.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    return X, available_features\n",
    "\n",
    "# Prepare features using the Account_pairing.ipynb approach\n",
    "X, feature_names = prepare_features_for_modeling_clustered(je_df)\n",
    "print(f\"\\\\nFeature matrix ready:\")\n",
    "print(f\"Shape: {X.shape}\")\n",
    "print(f\"Features summary:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ce569",
   "metadata": {},
   "source": [
    "### 4.3 PCA-based Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c079af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-Aware Isolation Forest (matching Account_pairing.ipynb methodology)\n",
    "def detect_anomalies_with_explanations_clustered(df, X, feature_names, contamination=0.01):\n",
    "    \"\"\"\n",
    "    Apply Isolation Forest within each cluster, matching the Account_pairing.ipynb approach.\n",
    "    Uses the same core features: Total_JE_Amount, infrequent_w, Total_JE_Amt_trailing0s\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get unique clusters\n",
    "    clusters = df['Cluster_ID'].unique()\n",
    "    print(f\"Applying Isolation Forest to {len(clusters)} clusters...\")\n",
    "    \n",
    "    for cluster_id in clusters:\n",
    "        cluster_mask = df['Cluster_ID'] == cluster_id\n",
    "        cluster_data = X[cluster_mask].copy()\n",
    "        cluster_df = df[cluster_mask].copy()\n",
    "        \n",
    "        if len(cluster_data) < 10:  # Skip small clusters\n",
    "            # Mark all as normal for small clusters\n",
    "            cluster_results = pd.DataFrame({\n",
    "                'JE_Doc_ID': cluster_df['JE_Doc_ID'],\n",
    "                'Cluster_ID': cluster_id,\n",
    "                'anomaly_score': 0,\n",
    "                'is_anomaly': 0,\n",
    "                'cluster_size': len(cluster_data)\n",
    "            })\n",
    "            results.append(cluster_results)\n",
    "            continue\n",
    "        \n",
    "        # Apply Isolation Forest within cluster\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        \n",
    "        # Standardize features within cluster\n",
    "        scaler = StandardScaler()\n",
    "        cluster_scaled = scaler.fit_transform(cluster_data)\n",
    "        \n",
    "        # Fit and predict\n",
    "        anomaly_labels = iso_forest.fit_predict(cluster_scaled)\n",
    "        anomaly_scores = iso_forest.decision_function(cluster_scaled)\n",
    "        \n",
    "        # Convert to binary (1 = anomaly, 0 = normal)\n",
    "        is_anomaly = (anomaly_labels == -1).astype(int)\n",
    "        \n",
    "        # Store results\n",
    "        cluster_results = pd.DataFrame({\n",
    "            'JE_Doc_ID': cluster_df['JE_Doc_ID'],\n",
    "            'Cluster_ID': cluster_id,\n",
    "            'anomaly_score': anomaly_scores,\n",
    "            'is_anomaly': is_anomaly,\n",
    "            'cluster_size': len(cluster_data)\n",
    "        })\n",
    "        \n",
    "        results.append(cluster_results)\n",
    "        \n",
    "        # Print cluster summary\n",
    "        anomaly_count = is_anomaly.sum()\n",
    "        if anomaly_count > 0:\n",
    "            print(f\"Cluster {cluster_id}: {anomaly_count}/{len(cluster_data)} anomalies detected\")\n",
    "    \n",
    "    # Combine results\n",
    "    final_results = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    total_anomalies = final_results['is_anomaly'].sum()\n",
    "    total_entries = len(final_results)\n",
    "    \n",
    "    print(f\"\\\\nOverall Results:\")\n",
    "    print(f\"Total anomalies detected: {total_anomalies}/{total_entries} ({total_anomalies/total_entries*100:.2f}%)\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "# Apply cluster-aware Isolation Forest\n",
    "print(\"Applying cluster-aware Isolation Forest (Account_pairing.ipynb methodology)...\")\n",
    "anomaly_results = detect_anomalies_with_explanations_clustered(je_df, X, feature_names, contamination=0.01)\n",
    "\n",
    "# Merge results back to main dataframe\n",
    "je_df = je_df.merge(\n",
    "    anomaly_results[['JE_Doc_ID', 'anomaly_score', 'is_anomaly', 'cluster_size']], \n",
    "    on='JE_Doc_ID', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\\\nAnomaly detection complete!\")\n",
    "print(f\"Anomalies by cluster size:\")\n",
    "cluster_anomaly_summary = je_df.groupby('cluster_size')['is_anomaly'].agg(['count', 'sum']).reset_index()\n",
    "cluster_anomaly_summary['anomaly_rate'] = (cluster_anomaly_summary['sum'] / cluster_anomaly_summary['count'] * 100).round(2)\n",
    "print(cluster_anomaly_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Detected Anomalies (Account_pairing.ipynb style)\n",
    "print(\"=== ANOMALY ANALYSIS RESULTS ===\")\n",
    "print()\n",
    "\n",
    "# Overall statistics\n",
    "total_jes = len(je_df)\n",
    "total_anomalies = je_df['is_anomaly'].sum()\n",
    "anomaly_rate = (total_anomalies / total_jes * 100)\n",
    "\n",
    "print(f\"📊 Overall Statistics:\")\n",
    "print(f\"   Total Journal Entries: {total_jes:,}\")\n",
    "print(f\"   Anomalies Detected: {total_anomalies:,}\")\n",
    "print(f\"   Anomaly Rate: {anomaly_rate:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Cluster-level analysis\n",
    "print(f\"🔍 Cluster Analysis:\")\n",
    "cluster_stats = je_df.groupby('Cluster_ID').agg({\n",
    "    'is_anomaly': ['count', 'sum'],\n",
    "    'Total_JE_Amount': ['mean', 'std'],\n",
    "    'Total_JE_Amt_trailing0s': 'mean',\n",
    "    'infrequent_w': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "cluster_stats.columns = ['Total_JEs', 'Anomalies', 'Avg_Amount', 'Std_Amount', 'Avg_Trailing_Zeros', 'Infrequent_Entries']\n",
    "cluster_stats['Anomaly_Rate_%'] = (cluster_stats['Anomalies'] / cluster_stats['Total_JEs'] * 100).round(2)\n",
    "\n",
    "# Show clusters with highest anomaly rates\n",
    "print(\"Top 10 Clusters by Anomaly Rate:\")\n",
    "top_anomaly_clusters = cluster_stats[cluster_stats['Total_JEs'] >= 5].nlargest(10, 'Anomaly_Rate_%')\n",
    "print(top_anomaly_clusters[['Total_JEs', 'Anomalies', 'Anomaly_Rate_%', 'Avg_Amount']])\n",
    "print()\n",
    "\n",
    "# Feature analysis for anomalies\n",
    "print(f\"📈 Feature Analysis of Anomalies:\")\n",
    "anomalies = je_df[je_df['is_anomaly'] == 1]\n",
    "normals = je_df[je_df['is_anomaly'] == 0]\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    feature_comparison = pd.DataFrame({\n",
    "        'Anomalies_Mean': anomalies[['Total_JE_Amount', 'Total_JE_Amt_trailing0s', 'infrequent_w', 'Line_Count']].mean(),\n",
    "        'Normal_Mean': normals[['Total_JE_Amount', 'Total_JE_Amt_trailing0s', 'infrequent_w', 'Line_Count']].mean(),\n",
    "    }).round(2)\n",
    "    feature_comparison['Difference'] = (feature_comparison['Anomalies_Mean'] - feature_comparison['Normal_Mean']).round(2)\n",
    "    print(feature_comparison)\n",
    "    print()\n",
    "\n",
    "# Top anomalies for review\n",
    "print(f\"🚨 Top 10 Anomalies for Priority Review:\")\n",
    "if len(anomalies) > 0:\n",
    "    top_anomalies = anomalies.nsmallest(10, 'anomaly_score')[\n",
    "        ['JE_Doc_ID', 'Cluster_ID', 'Total_JE_Amount', 'Total_JE_Amt_trailing0s', \n",
    "         'infrequent_w', 'anomaly_score', 'cluster_size']\n",
    "    ]\n",
    "    print(top_anomalies)\n",
    "else:\n",
    "    print(\"No anomalies detected with current parameters.\")\n",
    "print()\n",
    "\n",
    "# Summary by timing patterns\n",
    "print(f\"⏰ Timing Pattern Analysis:\")\n",
    "timing_analysis = je_df.groupby(['is_unusual_timing', 'is_anomaly']).size().unstack(fill_value=0)\n",
    "if not timing_analysis.empty:\n",
    "    timing_analysis['Total'] = timing_analysis.sum(axis=1)\n",
    "    timing_analysis['Anomaly_Rate_%'] = (timing_analysis.get(1, 0) / timing_analysis['Total'] * 100).round(2)\n",
    "    print(timing_analysis)\n",
    "print()\n",
    "\n",
    "print(\"=== MANUAL CLUSTERING APPLIED SUCCESSFULLY ===\")\n",
    "print(\"The anomaly detection now uses the same cluster-based methodology\")\n",
    "print(\"as the Account_pairing.ipynb notebook, focusing on account pairings\")\n",
    "print(\"and the key features: Total_JE_Amount, infrequent_w, Total_JE_Amt_trailing0s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b40db",
   "metadata": {},
   "source": [
    "## ✅ Manual Clustering Implementation Complete\n",
    "\n",
    "We have successfully applied the **exact same manual clustering methodology** from the Account_pairing.ipynb notebook to the BW dataset:\n",
    "\n",
    "### 🔑 Key Components Implemented:\n",
    "\n",
    "1. **Account Normalization**: Standardized account names using the same `normalize_name()` function\n",
    "2. **Account ID Creation**: Combined account numbers with normalized names (adapted for BW's segment structure)\n",
    "3. **Cluster Pattern Generation**: Created Dr/Cr + Account_ID combinations for each journal entry\n",
    "4. **Pattern-Based Grouping**: Grouped JEs by their account interaction patterns (same tuple-sorting logic)\n",
    "5. **Week-of-Month Calculation**: Applied the sophisticated 4-week smart merge algorithm\n",
    "6. **Weekly Frequency Analysis**: Calculated JE frequency per cluster per week with normalization\n",
    "7. **Trailing Zeros Detection**: Implemented fraud detection through round number analysis\n",
    "8. **JE-Level Summarization**: Aggregated line-level data to journal entry level\n",
    "9. **Cluster-Aware Anomaly Detection**: Applied Isolation Forest within each cluster using the same core features\n",
    "\n",
    "### 📊 Core Features Used (matching original):\n",
    "- **`Total_JE_Amount`**: Dollar amount of journal entry\n",
    "- **`infrequent_w`**: Binary flag for unusual timing within cluster \n",
    "- **`Total_JE_Amt_trailing0s`**: Count of trailing zeros (fraud indicator)\n",
    "\n",
    "### 🎯 Results:\n",
    "- **Manual clusters created** based on account pairing patterns\n",
    "- **Anomaly detection applied** within each cluster context\n",
    "- **Same methodology** as the sophisticated Account_pairing.ipynb approach\n",
    "- **Explainable results** with business context\n",
    "\n",
    "This implementation provides the **exact same level of sophistication** as the original notebook, adapted specifically for the BW dataset structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8415a",
   "metadata": {},
   "source": [
    "### 4.4 Combined Anomaly Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39035809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined anomaly indicators\n",
    "je_df['combined_anomaly'] = je_df['iso_anomaly'] | je_df['pca_anomaly']\n",
    "je_df['consensus_anomaly'] = je_df['iso_anomaly'] & je_df['pca_anomaly']\n",
    "\n",
    "# Create risk score (0-4 based on different indicators)\n",
    "risk_factors = [\n",
    "    'iso_anomaly',\n",
    "    'pca_anomaly', \n",
    "    'is_unusual_timing_first',\n",
    "    'is_backposted_any'\n",
    "]\n",
    "\n",
    "risk_factors_available = [col for col in risk_factors if col in je_df.columns]\n",
    "je_df['risk_score'] = je_df[risk_factors_available].sum(axis=1)\n",
    "\n",
    "print(\"Anomaly Detection Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Isolation Forest anomalies: {je_df['iso_anomaly'].sum()}\")\n",
    "print(f\"PCA anomalies: {je_df['pca_anomaly'].sum()}\")\n",
    "print(f\"Combined (either method): {je_df['combined_anomaly'].sum()}\")\n",
    "print(f\"Consensus (both methods): {je_df['consensus_anomaly'].sum()}\")\n",
    "print(f\"\\nRisk Score Distribution:\")\n",
    "print(je_df['risk_score'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298affc",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Risk Score Distribution\n",
    "je_df['risk_score'].value_counts().sort_index().plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Risk Score Distribution')\n",
    "axes[0,0].set_xlabel('Risk Score')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# Plot 2: Anomaly Scores Distribution\n",
    "axes[0,1].hist(je_df['iso_score'], bins=50, alpha=0.7, label='Isolation Forest')\n",
    "axes[0,1].axvline(je_df[je_df['iso_anomaly']==1]['iso_score'].min(), color='red', linestyle='--', label='Anomaly Threshold')\n",
    "axes[0,1].set_title('Isolation Forest Scores')\n",
    "axes[0,1].set_xlabel('Anomaly Score')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Plot 3: PCA Reconstruction Errors\n",
    "axes[1,0].hist(je_df['pca_error'], bins=50, alpha=0.7, label='PCA Reconstruction Error')\n",
    "axes[1,0].axvline(je_df[je_df['pca_anomaly']==1]['pca_error'].min(), color='red', linestyle='--', label='Anomaly Threshold')\n",
    "axes[1,0].set_title('PCA Reconstruction Errors')\n",
    "axes[1,0].set_xlabel('Reconstruction Error')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Plot 4: Feature Importance (if available)\n",
    "if len(feature_names) > 0:\n",
    "    # Get feature importance from isolation forest (approximation)\n",
    "    feature_importance = np.abs(iso_model.score_samples(iso_scaler.transform(X))).std(axis=0)\n",
    "    \n",
    "    # Plot top 10 features\n",
    "    top_features = pd.Series(feature_importance, index=feature_names).nlargest(10)\n",
    "    top_features.plot(kind='barh', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Top 10 Important Features')\n",
    "    axes[1,1].set_xlabel('Importance Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a89b7",
   "metadata": {},
   "source": [
    "## 6. Top Anomalies Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top anomalies for detailed analysis\n",
    "top_anomalies = je_df[\n",
    "    (je_df['risk_score'] >= 2) | \n",
    "    (je_df['consensus_anomaly'] == 1)\n",
    "].copy()\n",
    "\n",
    "# Sort by risk score and anomaly scores\n",
    "top_anomalies = top_anomalies.sort_values(['risk_score', 'iso_score'], ascending=[False, True])\n",
    "\n",
    "print(f\"Top {len(top_anomalies)} Anomalies for Review:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display key information for top anomalies\n",
    "display_cols = [\n",
    "    'risk_score', 'iso_anomaly', 'pca_anomaly',\n",
    "    'line_amount_sum', 'line_amount_count', 'trailing_zeros_sum'\n",
    "]\n",
    "\n",
    "# Add available timing and user columns\n",
    "timing_cols = [col for col in je_df.columns if col.endswith('_first') and ('timing' in col or 'backpost' in col or 'manual' in col)]\n",
    "display_cols.extend(timing_cols[:3])  # Add first 3 timing columns\n",
    "\n",
    "# Filter to existing columns\n",
    "available_cols = [col for col in display_cols if col in top_anomalies.columns]\n",
    "\n",
    "if len(top_anomalies) > 0:\n",
    "    print(top_anomalies[available_cols].head(10))\n",
    "else:\n",
    "    print(\"No high-risk anomalies found with current thresholds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b57de9",
   "metadata": {},
   "source": [
    "## 7. Business Rule-Based Risk Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_business_rules(df):\n",
    "    \"\"\"\n",
    "    Apply business rule-based risk scoring specific to the organization\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    business_risk_score = 0\n",
    "    \n",
    "    # Rule 1: Large round numbers (potential manipulation)\n",
    "    if 'line_amount_sum' in df.columns:\n",
    "        large_round = (df['line_amount_sum'] >= 10000) & (df['line_amount_sum'] % 1000 == 0)\n",
    "        df['rule_large_round'] = large_round.astype(int)\n",
    "        business_risk_score += df['rule_large_round']\n",
    "    \n",
    "    # Rule 2: High number of trailing zeros\n",
    "    if 'trailing_zeros_sum' in df.columns:\n",
    "        high_trailing_zeros = df['trailing_zeros_sum'] >= 3\n",
    "        df['rule_trailing_zeros'] = high_trailing_zeros.astype(int)\n",
    "        business_risk_score += df['rule_trailing_zeros']\n",
    "    \n",
    "    # Rule 3: Manual entries outside business hours\n",
    "    manual_unusual = (df.get('is_manual_entry_first', False)) & (df.get('is_unusual_timing_first', False))\n",
    "    df['rule_manual_unusual_timing'] = manual_unusual.astype(int)\n",
    "    business_risk_score += df['rule_manual_unusual_timing']\n",
    "    \n",
    "    # Rule 4: Backposted entries\n",
    "    if 'is_backposted_any' in df.columns:\n",
    "        df['rule_backposted'] = df['is_backposted_any'].astype(int)\n",
    "        business_risk_score += df['rule_backposted']\n",
    "    \n",
    "    # Rule 5: Reversal entries (require special attention)\n",
    "    if 'is_reversal_first' in df.columns:\n",
    "        df['rule_reversal'] = df['is_reversal_first'].astype(int)\n",
    "        business_risk_score += df['rule_reversal']\n",
    "    \n",
    "    df['business_risk_score'] = business_risk_score\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply business rules\n",
    "je_df = apply_business_rules(je_df)\n",
    "\n",
    "print(\"Business Rule-Based Risk Scoring:\")\n",
    "print(\"=\" * 40)\n",
    "business_rules = [col for col in je_df.columns if col.startswith('rule_')]\n",
    "for rule in business_rules:\n",
    "    triggered = je_df[rule].sum()\n",
    "    print(f\"{rule}: {triggered} entries triggered this rule\")\n",
    "\n",
    "print(f\"\\nBusiness Risk Score Distribution:\")\n",
    "print(je_df['business_risk_score'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad4bf74",
   "metadata": {},
   "source": [
    "## 8. Final Risk Assessment and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d429b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final combined risk assessment\n",
    "je_df['final_risk_score'] = (\n",
    "    je_df['risk_score'] * 0.4 +  # ML-based anomaly detection (40%)\n",
    "    je_df['business_risk_score'] * 0.6  # Business rule-based (60%)\n",
    ")\n",
    "\n",
    "# Categorize risk levels\n",
    "def categorize_risk(score):\n",
    "    if score >= 3:\n",
    "        return 'High'\n",
    "    elif score >= 1.5:\n",
    "        return 'Medium'\n",
    "    elif score >= 0.5:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Minimal'\n",
    "\n",
    "je_df['risk_category'] = je_df['final_risk_score'].apply(categorize_risk)\n",
    "\n",
    "print(\"Final Risk Assessment:\")\n",
    "print(\"=\" * 30)\n",
    "risk_summary = je_df['risk_category'].value_counts()\n",
    "print(risk_summary)\n",
    "\n",
    "print(f\"\\nRecommendations for Review:\")\n",
    "print(\"=\" * 30)\n",
    "high_risk = risk_summary.get('High', 0)\n",
    "medium_risk = risk_summary.get('Medium', 0)\n",
    "print(f\"Priority 1 - High Risk: {high_risk} entries (immediate review required)\")\n",
    "print(f\"Priority 2 - Medium Risk: {medium_risk} entries (review within 1 week)\")\n",
    "\n",
    "# Export high and medium risk entries\n",
    "priority_entries = je_df[je_df['risk_category'].isin(['High', 'Medium'])].copy()\n",
    "\n",
    "if len(priority_entries) > 0:\n",
    "    # Select key columns for export\n",
    "    export_cols = ['final_risk_score', 'risk_category', 'iso_anomaly', 'pca_anomaly']\n",
    "    \n",
    "    # Add amount and timing info\n",
    "    amount_cols = [col for col in je_df.columns if 'line_amount' in col]\n",
    "    timing_cols = [col for col in je_df.columns if any(x in col for x in ['timing', 'backpost', 'posted_date'])]\n",
    "    \n",
    "    export_cols.extend(amount_cols[:3])  # First 3 amount columns\n",
    "    export_cols.extend(timing_cols[:3])  # First 3 timing columns\n",
    "    \n",
    "    # Filter to existing columns\n",
    "    available_export_cols = [col for col in export_cols if col in priority_entries.columns]\n",
    "    \n",
    "    priority_export = priority_entries[available_export_cols]\n",
    "    \n",
    "    # Save to CSV\n",
    "    priority_export.to_csv('BW_Priority_Anomalies_for_Review.csv')\n",
    "    print(f\"\\nExported {len(priority_export)} priority entries to 'BW_Priority_Anomalies_for_Review.csv'\")\n",
    "    \n",
    "    # Display top 5 highest risk entries\n",
    "    print(f\"\\nTop 5 Highest Risk Entries:\")\n",
    "    top_5 = priority_entries.nlargest(5, 'final_risk_score')[available_export_cols]\n",
    "    print(top_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f43dc",
   "metadata": {},
   "source": [
    "## 9. Model Performance and Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a035e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "print(\"Feature Correlation with Risk Scores:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "numeric_features_available = [col for col in feature_names if col in je_df.columns]\n",
    "\n",
    "if len(numeric_features_available) > 0:\n",
    "    correlations = je_df[numeric_features_available + ['final_risk_score']].corr()['final_risk_score'].sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Features Correlated with Risk Score:\")\n",
    "    print(correlations.head(11)[:-1])  # Exclude self-correlation\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_10_corr = correlations.head(11)[:-1]  # Top 10, excluding self\n",
    "    top_10_corr.plot(kind='barh')\n",
    "    plt.title('Top 10 Features Correlated with Final Risk Score')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary statistics by risk category\n",
    "print(f\"\\nSummary Statistics by Risk Category:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "key_metrics = ['line_amount_sum', 'line_amount_count', 'trailing_zeros_sum']\n",
    "available_metrics = [col for col in key_metrics if col in je_df.columns]\n",
    "\n",
    "if len(available_metrics) > 0:\n",
    "    risk_stats = je_df.groupby('risk_category')[available_metrics].agg(['mean', 'median', 'std'])\n",
    "    print(risk_stats)\n",
    "\n",
    "print(f\"\\nAnomaly Detection Complete!\")\n",
    "print(f\"Total Journal Entries Analyzed: {len(je_df)}\")\n",
    "print(f\"High Priority Entries for Review: {len(je_df[je_df['risk_category'] == 'High'])}\")\n",
    "print(f\"Medium Priority Entries for Review: {len(je_df[je_df['risk_category'] == 'Medium'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
